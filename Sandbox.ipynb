{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Sandbox.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNxoL/8g9QazFgjmVPgq9l6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/D-Sokol/Coursera-Spreadsheet/blob/master/Sandbox.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q-l-0OeQjXhO",
        "outputId": "364e236a-7f93-41d2-f13e-6cffae068c45"
      },
      "source": [
        "!pip install transformers==4.1.1"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers==4.1.1 in /usr/local/lib/python3.6/dist-packages (4.1.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers==4.1.1) (20.8)\n",
            "Requirement already satisfied: tokenizers==0.9.4 in /usr/local/lib/python3.6/dist-packages (from transformers==4.1.1) (0.9.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers==4.1.1) (2019.12.20)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers==4.1.1) (0.0.43)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers==4.1.1) (0.8)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers==4.1.1) (3.0.12)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers==4.1.1) (1.19.4)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers==4.1.1) (4.41.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers==4.1.1) (2.23.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers==4.1.1) (2.4.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==4.1.1) (1.0.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==4.1.1) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==4.1.1) (7.1.2)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==4.1.1) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==4.1.1) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==4.1.1) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==4.1.1) (2020.12.5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ieG-Qb3Mh0Hd"
      },
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rwTnT3neKWJW"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aVOB60vVjPju"
      },
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "tokenizer = GPT2Tokenizer.from_pretrained('gpt2', add_prefix_space=True)\n",
        "model = GPT2LMHeadModel.from_pretrained('gpt2').train(False).to(device)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l1GOGGfmjhq6",
        "outputId": "43c0e7ab-aa9d-4fe7-ca22-9a8d5ad3aa8b"
      },
      "source": [
        "start_text = \"The best example for denotarikon starts with\"\n",
        "start_tokens = tokenizer.encode(start_text)\n",
        "start_tokens"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[383, 1266, 1672, 329, 2853, 313, 283, 1134, 261, 4940, 351]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KlRDYccBmg9y"
      },
      "source": [
        "tokens = start_tokens[:]\n",
        "with torch.no_grad():\n",
        "    result = model(torch.tensor(tokens, device=device)[None], past_key_values=None)\n",
        "    next_logits, past = result['logits'][0, -1, :], result['past_key_values']\n",
        "    for i in range(100):\n",
        "        next_probas = torch.softmax(next_logits, dim=-1).cpu()\n",
        "        # TODO: use actual random instead of the most probable token\n",
        "        next_ix = next_probas.argmax()\n",
        "        tokens.append(next_ix.item())\n",
        "\n",
        "        # TODO: split by 1st letter\n",
        "        result = model(next_ix[None], past_key_values=past)\n",
        "        next_logits, past = result['logits'][0, :], result['past_key_values']\n",
        "        next_probas = torch.softmax(next_logits, dim=-1).cpu()"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IAyDvGAzPC_P",
        "outputId": "78a71ba9-a2a0-4dc7-e9ed-17e75562de3e"
      },
      "source": [
        "print(tokenizer.decode(tokens))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " The best example for denotarikon starts with the following:\n",
            "\n",
            "The first thing to notice is that the first two lines of the first line are not the same. The first two lines are the same.\n",
            "\n",
            "The second line is the same.\n",
            "\n",
            "The third line is the same.\n",
            "\n",
            "The fourth line is the same.\n",
            "\n",
            "The fifth line is the same.\n",
            "\n",
            "The sixth line is the same.\n",
            "\n",
            "The seventh line is the same.\n",
            "\n",
            "The eighth line is the same.\n",
            "\n",
            "The ninth\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}